{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5026615a",
   "metadata": {},
   "source": [
    "# Computational Social Science Project #2 \n",
    "\n",
    "*Group number:* \n",
    "\n",
    "*Group members:*   \n",
    "Rachel Pizatella-Haswell, Brenda Sciepura, Omair Gil\n",
    "\n",
    "*Semester:* Fall 2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceabce6",
   "metadata": {},
   "source": [
    "Below we fill in some of the code you might use to answer some of the questions. Here are some additional resources for when you get stuck:\n",
    "* Code and documentation provided in the course notebooks  \n",
    "* [Markdown cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) to help with formatting the Jupyter notebook\n",
    "* Try Googling any errors you get and consult Stack Overflow, etc. Someone has probably had your question before!\n",
    "* Send me a pull request on GitHub flagging the syntax that's tripping you up "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7920223d",
   "metadata": {},
   "source": [
    "## 1. Introduction/Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013cd72b",
   "metadata": {},
   "source": [
    "#### a) Import relevant libraries\n",
    "Add the other libraries you need for your code below and/or as you go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca2b1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries you might need here \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "#Import specific sklearn functions\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import feature_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# use random seed for consistent results \n",
    "np.random.seed(273)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b23b0",
   "metadata": {},
   "source": [
    "#### b) Read in and inspect data frame \n",
    "Read in the data frame and look at some of its attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b21e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171cd9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv(\"Diabetes with Population Info by County 2017.csv\", \n",
    "                       #CountyFips needs to be a string so leading 0 isn't dropped (this is only if you want to make choropleth map): \n",
    "                       dtype={\"CountyFIPS\": str}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the dimensions of the diabetes data frame\n",
    "print('shape: ', diabetes.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91328e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100) # tells pandas how many rows to display when printing so results don't get truncated\n",
    "\n",
    "# look at the data types for each column in diabetes df \n",
    "print('data types:', diabetes.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e77f8b",
   "metadata": {},
   "source": [
    "Immediately, we see that some of the features that should be numeric (e.g., Diabetes_Number, Obesity_Number,  and Physical_Inactivity_Number) are not. We can check to see what the non-numeric values are in a column where we are expecting numeric information with a combination of `str.isnumeric()` and `unique()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72315139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return rows where the column \"Diabetes_Number\" is non-numeric and get the unique values of these rows\n",
    "# the \"~\" below in front of diabetes negates the str.isnumeric() so it only takes non-numeric values\n",
    "print(diabetes[~diabetes[\"Diabetes_Number\"].str.isnumeric()][\"Diabetes_Number\"].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a749fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes[~diabetes[\"Diabetes_Number\"].str.isnumeric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e47c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the same as above, but for \"Obesity_Number\" :\n",
    "print(diabetes[~diabetes[\"Obesity_Number\"].str.isnumeric()][\"Obesity_Number\"].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e33f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes['sex and age_total population_65 years and over_sex ratio (males per 100 females)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b705636f",
   "metadata": {},
   "source": [
    "The values contained in the two columns above making them objects (rather than integers) appear to be strings like \"No Data\" and \"Suppressed.\" Let's drop those rows in the next section, and also recode Physical_Inactivity_Number to be an integer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a442a",
   "metadata": {},
   "source": [
    "#### c. Recode variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31fe962",
   "metadata": {},
   "source": [
    "Convert 'Diabetes_Number', 'Obesity_Number', and 'Physical_Inactivity_Number' to integers below so we can use them in our analysis. Also fill in the object type we want to recode 'sex and age_total population_65 years and over_sex ratio (males per 100 females)' to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diabetes\n",
    "# keep only useful info about our target feature, i.e., where diabetes_number not = 'Suppressed'\n",
    "diabetes = diabetes[diabetes['Diabetes_Number']!=\"Suppressed\"]  # note that the inside reference to the diabetes df identifies the column, and the outer calls specific rows according to a condition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328824db",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not (diabetes['Diabetes_Number'] == 'Suppressed').any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e399e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = diabetes[diabetes['Obesity_Number']!='No Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115eea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes[['Diabetes_Number', 'Obesity_Number', 'Physical_Inactivity_Number']] = diabetes[['Diabetes_Number', 'Obesity_Number', 'Physical_Inactivity_Number']].astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297142eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes[['Diabetes_Number', 'Obesity_Number', 'Physical_Inactivity_Number']].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff6cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 65+ sex ratio had one \"-\" in it so let's drop that row first\n",
    "diabetes = diabetes[diabetes['sex and age_total population_65 years and over_sex ratio (males per 100 females)']!= \"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes['sex and age_total population_65 years and over_sex ratio (males per 100 females)'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4acf34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to numeric (specifically, integer or float?) from string (because originally included the \"-\" )\n",
    "diabetes['sex and age_total population_65 years and over_sex ratio (males per 100 females)'] = diabetes['sex and age_total population_65 years and over_sex ratio (males per 100 females)'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0cbdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes['total housing units']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a6850c",
   "metadata": {},
   "source": [
    "We should probably scale our count variables to be proportional to county population. We create the list 'rc_cols' to select all the features we want to rescale, and then use the `.div()` method to avoid typing out every single column we want to recode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bcbacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select count variables to rc to percentages; make sure we leave out ratios and our population variable b/c these don't make sense to scale by population\n",
    "rc_cols = [col for col in diabetes.columns if col not in ['County', 'State', 'CountyFIPS', \n",
    "                                                        'sex and age_total population_65 years and over_sex ratio (males per 100 females)', 'sex and age_total population_sex ratio (males per 100 females)', 'sex and age_total population_18 years and over_sex ratio (males per 100 females)',  \n",
    "                                                        'race_total population', 'total housing units',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5583491",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes[rc_cols] = diabetes[rc_cols].apply(pd.to_numeric, errors='coerce') # recode all selected columns to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb2299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide all columns but those listed above by total population to calculate rates\n",
    "diabetes[rc_cols] = diabetes[rc_cols].div(diabetes['race_total population'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625966df",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e1fe2",
   "metadata": {},
   "source": [
    "Let's check our work. Are all rates bounded by 0 and 1 as expected? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# inspect recoded values\n",
    "diabetes_summary = diabetes.describe().transpose() # note we use the transpose method rather than .T because this object is not a numpy array\n",
    "diabetes_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check recoding \n",
    "with pd.option_context('display.max_rows', 100, 'display.max_columns', None): \n",
    "    display(diabetes_summary.iloc[ : ,[0,1,3,7]]) # select which columns in the summary table we want to present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be50bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in rc_cols:\n",
    "    condition = (diabetes[col] >= 0) & (diabetes[col] <= 1)\n",
    "    assert condition.all(), f\"Not all values in {col} are bounded by 0 and 1.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dcc070",
   "metadata": {},
   "source": [
    "#### d. Check for duplicate columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acce0aa",
   "metadata": {},
   "source": [
    "There are a lot of columns in this data frame. Let's see if there are any are duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700163cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used Google to figure this out, and adapted this example for our purposes:  \n",
    "# source: https://thispointer.com/how-to-find-drop-duplicate-columns-in-a-dataframe-python-pandas/ \n",
    "def getDuplicateColumns(df):\n",
    "    '''\n",
    "    Get a list of duplicate columns.\n",
    "    It will iterate over all the columns in dataframe and find the columns whose contents are duplicate.\n",
    "    :param df: Dataframe object\n",
    "    :return: List of columns whose contents are duplicates.\n",
    "    '''\n",
    "    duplicateColumnNames = set()\n",
    "    # Iterate over all the columns in dataframe\n",
    "    for x in range(df.shape[1]):\n",
    "        # Select column at xth index.\n",
    "        col = df.iloc[:, x]\n",
    "        # Iterate over all the columns in DataFrame from (x+1)th index till end\n",
    "        for y in range(x + 1, df.shape[1]):\n",
    "            # Select column at yth index.\n",
    "            otherCol = df.iloc[:, y]\n",
    "            # Check if two columns at x 7 y index are equal\n",
    "            if col.equals(otherCol):\n",
    "                duplicateColumnNames.add(df.columns.values[y])\n",
    "    return list(duplicateColumnNames)\n",
    "\n",
    "duplicateColumnNames = list(getDuplicateColumns(diabetes))\n",
    "print('Duplicate Columns are as follows: ')\n",
    "duplicateColumnNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f2da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.loc[:,['hispanic or latino and race_total population',\n",
    " 'sex and age_total population_18 years and over_1',\n",
    " 'race_total population_two or more races_1',\n",
    " 'sex and age_total population_65 years and over_1',\n",
    " 'sex and age_total population',\n",
    " 'race_total population_one race_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now drop list of duplicate features from our df using the .drop() method\n",
    "diabetes = diabetes.drop(columns=duplicateColumnNames) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe3e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary mapping states to regions\n",
    "state_to_region = {\n",
    "    'Alabama': 'Southeast',\n",
    "    'Alaska': 'West',\n",
    "    'Arizona': 'West',\n",
    "    'Arkansas': 'South',\n",
    "    'California': 'West',\n",
    "    'Colorado': 'West',\n",
    "    'Connecticut': 'Northeast',\n",
    "    'Delaware': 'Northeast',\n",
    "    'District of Columbia': 'Southeast',\n",
    "    'Florida': 'Southeast',\n",
    "    'Georgia': 'Southeast',\n",
    "    'Hawaii': 'West',\n",
    "    'Idaho': 'West',\n",
    "    'Illinois': 'Midwest',\n",
    "    'Indiana': 'Midwest',\n",
    "    'Iowa': 'Midwest',\n",
    "    'Kansas': 'Midwest',\n",
    "    'Kentucky': 'South',\n",
    "    'Louisiana': 'South',\n",
    "    'Maine': 'Northeast',\n",
    "    'Maryland': 'Northeast',\n",
    "    'Massachusetts': 'Northeast',\n",
    "    'Michigan': 'Midwest',\n",
    "    'Minnesota': 'Midwest',\n",
    "    'Mississippi': 'South',\n",
    "    'Missouri': 'Midwest',\n",
    "    'Montana': 'West',\n",
    "    'Nebraska': 'Midwest',\n",
    "    'Nevada': 'West',\n",
    "    'New Hampshire': 'Northeast',\n",
    "    'New Jersey': 'Northeast',\n",
    "    'New Mexico': 'West',\n",
    "    'New York': 'Northeast',\n",
    "    'North Carolina': 'Southeast',\n",
    "    'North Dakota': 'Midwest',\n",
    "      'Ohio': 'Midwest',\n",
    "    'Oklahoma': 'South',\n",
    "    'Oregon': 'West',\n",
    "    'Pennsylvania': 'Northeast',\n",
    "    'Rhode Island': 'Northeast',\n",
    "    'South Carolina': 'Southeast',\n",
    "    'South Dakota': 'Midwest',\n",
    "    'Tennessee': 'South',\n",
    "    'Texas': 'South',\n",
    "    'Utah': 'West',\n",
    "    'Vermont': 'Northeast',\n",
    "    'Virginia': 'Southeast',\n",
    "    'Washington': 'West',\n",
    "    'West Virginia': 'South',\n",
    "    'Wisconsin': 'Midwest',\n",
    "    'Wyoming': 'West'\n",
    "}\n",
    "\n",
    "# Add a new 'Region' column based on the mapping\n",
    "diabetes['Region'] = diabetes['State'].map(state_to_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print to verify'Region' column has been added\n",
    "diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4007b8d1",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf4f76",
   "metadata": {},
   "source": [
    "### Exploring geographic variation in diabetes prevalence\n",
    "Because we are interested in understanding which counties to target for a diabetes prevention program, the geographic variation in diabetes prevalence is of particular importance. Depending on the aims of the policymaker, we may be interested in targeting regions that have multiple counties with high diabetes prevalence. Introducing the prevention program in a high risk region could have the benefit of better utilizing the fixed costs that would be necessary toget the program up and running in order eventually scale up and reach more counties. Furthermore, we might expect that the benefits of a successful program pilot could spill over to neighboring counties with similarly high diabetes prevalence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86d3bd",
   "metadata": {},
   "source": [
    "### Read in US county shapefile from the UC census and merge to the diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('cb_2018_us_county_500k.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[gdf['STATEFP'] == \"01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2e13f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['CountyFIPS'] = gdf['STATEFP'] + gdf['COUNTYFP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f263dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = diabetes.merge(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af87e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = gdf.set_index('CountyFIPS').join(diabetes.set_index('CountyFIPS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09242bc9",
   "metadata": {},
   "source": [
    "### Plot geographic variation in diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c25148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot figure of diabetes percentage across counties\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "merged_df.plot(column='Diabetes_Number', cmap='magma_r', linewidth=0.01, ax=ax, edgecolor='0.8')\n",
    "ax.axis('off')\n",
    "plt.xlim(-130, -65)\n",
    "plt.ylim(25, 55) \n",
    "ax.set_title('Diabetes Rate by U.S. Counties')\n",
    "sm = plt.cm.ScalarMappable(cmap='magma_r', norm=plt.Normalize(vmin=merged_df['Diabetes_Number'].min(), vmax=merged_df['Diabetes_Number'].max()))\n",
    "sm._A = []\n",
    "cbar = plt.colorbar(sm)\n",
    "\n",
    "# Set the label for the colorbar\n",
    "cbar.set_label('Percentage Label')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab91c0",
   "metadata": {},
   "source": [
    "### Get summary statistics for diabetes rates across counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3acaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes['Diabetes_Number'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5174d2d1",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Diabetes varies significantly from county to county. County prevalence ranges from just 2% to 22%. The map shows that counties with high diabetes rates tend to be highly clustered. Such clusters exist throughout the midwest and western states, but counties with high diabetes rates are most prevalent in the south. Consequently, policymakers might want to focus on the southern states  for the roll out of the pilot program in order to learn how to combat diabetes in areas where it is most concentrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defd5832",
   "metadata": {},
   "source": [
    "## Investigate relationship between Hispanic population and diabetes\n",
    "The following chart shows how diabetes prevalence varies with the percentage of a county that is Hispanic or Latino. Understanding the demographic makeup of counties with high diabetes rate is crucial for treating and preventing the disease. Understanding if diabetes tends to be high among certain ethnic groups or ages can help policymakers to construct prevention programs that are culturally appropriate and that work in partnership with the right community organizations and leaders. \n",
    "\n",
    "The graph below shows a weak relationship between the percentage of a county that identifies as Hispanic/Latino and the diabetes rate. Furthermore, the correlation is slightly negative. This indicates that prevention of diabetes among Hispanic/Latino populations might not warrant special consideration for ploiycmakers. Note that this is an average of the relationship and that there are some counties with both large Hispanic/Latino poulations and high diabetes prevalence. Consequently, the demographic makeup and cultural context of each county should be considered in turn when rolling out prevention programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc3488",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = diabetes['hispanic or latino and race_total population_hispanic or latino (of any race)'],\n",
    "           y = diabetes['Diabetes_Number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df05b0",
   "metadata": {},
   "source": [
    "## 3. Prepare to Fit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73492869",
   "metadata": {},
   "source": [
    "### 3.1 Finalize Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67257021",
   "metadata": {},
   "source": [
    "We've already cleaned up the data, but we can make a few more adjustments before partitioning the data and training models. Let's recode 'State' to be a categorical variable using `pd.get_dummies` and drop 'County' using `.drop()` because 'CountyFIPS' is already a unique identifier for the county. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05595833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy features out of 'State' , which might be related to diabetes rates \n",
    "diabetes_clean = pd.get_dummies(diabetes, \n",
    "                               columns = ['Region'],  \n",
    "                               drop_first = True) # only create 49 dummies by dropping first in category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5496a9e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert diabetes_clean['race alone or in combination with one or more other races_total population'].all() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa1676",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# drop 'County' variable\n",
    "diabetes_clean = diabetes_clean.drop(labels = ['County'],\n",
    "                               axis = 1) # which axis tells python we want to drop columns rather than index rows?\n",
    "\n",
    "# look at first 10 rows of new data frame \n",
    "diabetes_clean.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768ecd6",
   "metadata": {},
   "source": [
    "### 3.2/3.3 Partition Data and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305051f",
   "metadata": {},
   "source": [
    "Now, we will partition our data to prepare it for the training process. We will use 60% trainâ€”20% validationâ€”20% test in this case. More data in the training set lowers bias, but then increases variance in the validation/test sets. Balancing between bias and variance with choice of these set sizes is important as we want to ensure that there is enough data to train on to get good predictions, but also want to make sure our hold-out sets are representative enough. This case is difficult since we only have a few thousand observations. It would help if we had more years worth of data.\n",
    "\n",
    "The training set is the portion of the data we use to fit our model. The alogirthm will solve the parameters of the objective function using the observations in the training data. In the case of OLS, this is the vector $\\beta$ that minimizes the residual sum of squares. Formally, we solve the following criterion function using the training data:\n",
    "$$\n",
    "\\min_{\\beta} \\sum_{i=1}^n (y_i - \\beta x_{i})^2\n",
    "$$\n",
    "\n",
    "The validation set is used to test how well the fitted model works with different tuning parameters. Testing the model in an interim stage helps us to assess how to set tuning parameters such as the rate of regularization in the case of Ride or LASSO in order to ensure that the fitted model generalizes well to out of sample data.\n",
    "\n",
    "The test set is the final portion of the data used to formally assess how well our fitted algorithm performs on non-training data. This is intended to capture how well the model will work when we take it to new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceacdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set y \n",
    "y = diabetes_clean['Diabetes_Number']\n",
    "\n",
    "# X (everything except diabetes, our target)\n",
    "X = diabetes_clean.drop(columns = ['Diabetes_Number', 'CountyFIPS', 'State'])\n",
    "features_df = diabetes_clean.drop(columns = ['Diabetes_Number', 'State', 'CountyFIPS', 'race alone or in combination with one or more other races_total population'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de312e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ada991",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = feature_selection.VarianceThreshold(0)\n",
    "X = selector.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d908580",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec300cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "# train_test_split returns 4 values: X_train, X_test, y_train, y_test, so how do we create a 60-20-20 train-validate-test split? \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76555cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X[:,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a2918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now split the training sample into a validation and a true training set\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size = .25, random_state = 456)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f213c",
   "metadata": {},
   "source": [
    "We will want to standardize our data to be mean centered and have unit variance. Forcing all features to be on the same scale helps to ensure that no one feature dominates the objective function. This ensures that the algorithm can learn from all of the features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e32ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3276e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_validate = scaler.transform(X_validate)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38acf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X_train[:,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acab910",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X_validate[:,25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e762cb",
   "metadata": {},
   "source": [
    "## 4. Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e530a6",
   "metadata": {},
   "source": [
    "### 4.1 Describe the selected models\n",
    "\n",
    "### OLS\n",
    "OLS fits a linear model to the data that minimizes the sum of squared errors, or the sum of squared differences between the model prediction and the actual observations. OLS assumes that the true relationship between the regressors and the independent variable is linear. OLS may poorly fit the data if the true relationship is not linear. One advantage is that the coefficients from OLS are easily interpretable. Under OLS, terms have an additive correlation with the outcome of interest and a one unit change in a regressor has the same effect on the outcome no matter the level of the regressor. \n",
    "\n",
    "\n",
    "### RIDGE\n",
    "Similar to OLS, ridge regression fits a linear model (technically it does not need to be linear as you could have ridge for logit or other non-linear models). The objective function of the ridge regression is to minimize the sum of squared residuals plus a term that depends on the absolute size of each coeffcient subject to a tuning parameter. In essence, this prevents any coefficient $\\beta_j$ from being too large if the relevant predictor does not explain a sufficient amount of the variation in the outcome variable. Overall, this has the effect of minimizing the coefficients on unimportant predictors which reduces overfitting. One con to ridge regression is that coefficients are less interpretable. Also, the ridge regression will include all the covariates in the final estimate instead of just including the important covariates as opposed to LASSO. Additionally, the ridge regression will introduce bias into the coefficient estimates. Some predictors may not explain a significant portion of the variation in the outcome, but nonethless may be important correlates between a specific predictor of interest and the outcome simultaneously. This will bias the causal interpretation of the coefficient on that predictor of interest.\n",
    "\n",
    "### LASSO\n",
    "The LASSO works similar to the ridge regression in that it minimizes the sum of squared residuals subject to a penalty on the size of the coefficients. But where the ridge regression penalizes the squared size of the coefficients, the LASSO penalizes the absolute size. Because the absolute value has a kink at 0, many potential predictors do not explain sufficient variation in the outcome to justify their inclusion in the model. This often results in LASSO only selecting a subset of the potential predictors. Similar to ridge, the coefficients on the LASSO are less interpretable than OLS and the use of regularization introduces extra bias as the cost of lowering the out-of-sample variance in predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a87b5a6",
   "metadata": {},
   "source": [
    "### 4.2 Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b0f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train OLS \n",
    "ols_reg = linear_model.LinearRegression()\n",
    "ols_reg.fit(X_train, y_train)\n",
    "diabetes_ols_pred = ols_reg.predict(X_validate)\n",
    "\n",
    "ols_train_pred = ols_reg.predict(X_train)\n",
    "ssr = np.sum((y_train - ols_train_pred)**2)\n",
    "sst = np.sum((y_train - np.mean(y_train))**2)\n",
    "manual_ols_r2 = 1 - (ssr/sst)\n",
    "canned_ols_r2 = r2_score(y_train, ols_train_pred)\n",
    "assert canned_ols_r2 == manual_ols_r2\n",
    "manual_ols_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e20037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Lasso\n",
    "lasso_reg = Lasso(alpha=.001)\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "lasso_train_pred = lasso_reg.predict(X_train)\n",
    "lasso_train_r2 = r2_score(y_train, lasso_train_pred)\n",
    "lasso_train_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f92d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_lasso_pred = lasso_reg.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d7d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the non-zero coefficients and see what variables are being selected\n",
    "features_df.iloc[:,lasso_reg.coef_!=0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Ridge \n",
    "ridge_reg = Ridge(alpha=.001)\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "ridge_train_pred = ridge_reg.predict(X_train)\n",
    "ridge_train_r2 = r2_score(y_train, ridge_train_pred)\n",
    "ridge_train_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e868d1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_ridge_pred = ridge_reg.predict(X_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa599fef",
   "metadata": {},
   "source": [
    "## 5. Validate and Refine Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05461fd",
   "metadata": {},
   "source": [
    "### 5.1 Test the models on the validation set \n",
    "Manually calculate the test MSE then use the canned sklearn function to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS\n",
    "manual_ols_validate_mse = np.mean((y_validate - diabetes_ols_pred)**2)\n",
    "canned_ols_validate_mse = mean_squared_error(y_validate, diabetes_ols_pred)\n",
    "assert manual_ols_validate_mse == canned_ols_validate_mse\n",
    "canned_ols_validate_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f242f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LASSO \n",
    "manual_lasso_validate_mse = np.mean((y_validate - diabetes_lasso_pred)**2)\n",
    "canned_lasso_validate_mse = mean_squared_error(y_validate, diabetes_lasso_pred)\n",
    "assert manual_lasso_validate_mse == canned_lasso_validate_mse\n",
    "canned_lasso_validate_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge\n",
    "manual_ridge_validate_mse = np.mean((y_validate - diabetes_ridge_pred)**2)\n",
    "canned_ridge_validate_mse = mean_squared_error(y_validate, diabetes_ridge_pred)\n",
    "assert manual_ridge_validate_mse == canned_ridge_validate_mse\n",
    "canned_ridge_validate_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc37f63",
   "metadata": {},
   "source": [
    "The lasso had the lowest mean squared error in the validation set. OLS, Ridge, and LASSO all performed comparably which is unsurprising given that we didn't add interaction terms or nonlinear terms. Adding these features may have caused ols to overfit the training data and perform poorly out of sample. Adding polynomials and interaciton terms would have made this a problem more suited to the regularization approaches of LASSO and Ridge that help to prevent overfitting by constraining the flexibility of the model. Since these features were not included and lambda was set very low for both the Ridge and LASSO, we should expect these approaches to perform similarly to OLS.\n",
    "\n",
    "That LASSO performed slightly better than OLS out of sample is an indication that OLS slightly overfit the training sample relative to LASSO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02711ee5",
   "metadata": {},
   "source": [
    "### 5.2 Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43158bde",
   "metadata": {},
   "source": [
    "I'm a little confused by this part. Why would we select out the unimportant features and retrain the model? This is already what LASSO is doing. What does this add?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38858754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unimportant features\n",
    "X_train_selected = X_train[:,lasso_reg.coef_!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6a0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-train Lasso\n",
    "lasso_reg_selected = Lasso(alpha=.001)\n",
    "lasso_reg_selected.fit(X_train_selected, y_train)\n",
    "lasso_selected_pred = lasso_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f4a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate test MSE\n",
    "lasso_test_mse = mean_squared_error(y_test, lasso_selected_pred)\n",
    "lasso_test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate test R^2\n",
    "lasso_test_r2 = r2_score(y_test, lasso_selected_pred)\n",
    "lasso_test_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdafe17e",
   "metadata": {},
   "source": [
    "Our LASSO specification can explain a little under 48% of the variation in diabetes prevalence by county in the test data set. This is a significant amount of the overall variation given the limited number of possible features. One advantage of using both the validation and test sets in a public policy application is that the cost of implementing a policy based on an innacruate algorithm may be quite high. If the policymaker is risk-averse, we want to be confident that our predictions will generalize well to other settings. Using both the test and validation sets helps to ensure that our out of sample error rate is adequate enough to justify using the algorithm for policy decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e783e3df",
   "metadata": {},
   "source": [
    "### 5.3 Implement a Cross-Validation Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bbab9e",
   "metadata": {},
   "source": [
    "I will use cross validation to both select the value of the LASSO tuning parameter (lambda) a validation error rate. Implementing a CV approach to find the best lambda is a bad idea if we are concerned about specifying the correct model or saying something causal about diabetes prevalance. This is because a CV-selected lambda will optimize the prediction power of the model, but may shrink coefficients on important variables to zero. Here, we are only concerned with predicting the rate of diabetes in a county. Using a cross-validation approach to select the tuning parameter is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046baa3",
   "metadata": {},
   "source": [
    "Because we are using cross validation, we can take advantage of the full size of the training set. Rather than split the data into dedicated train/validate/test partiitions we can just perform a train/test split. The cross validation approach will take advantage of the full set of training data to both train and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0235ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv_train, X_cv_test, y_cv_train, y_cv_test = train_test_split(X, y, test_size=.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad3403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to re-standardize\n",
    "cv_scaler = StandardScaler().fit(X_cv_train) \n",
    "X_cv_train = cv_scaler.transform(X_cv_train)\n",
    "X_cv_test = cv_scaler.transform(X_cv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac9756",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cv_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ad8c7d",
   "metadata": {},
   "source": [
    "### Choosing number of folds for cross validation\n",
    "The choice of folds when performing k-fold cross validation is a trade-off between bias and variance. The more folds there are, the fewer observations there will be in each fold. When the data is trained on K-1 folds with large k, the k-1 folds will contain most of the training data. Containing more data means that the bias will be lower as the algorithm can more accurately fit the data. However, this means that overfitting may be a problem as large k more closely approximates a leave-one-out cross validation approach where prediction on the single observation that is left out may suffer from high variance as any single observation may not be typical of the underlying distribtion of data.\n",
    "\n",
    "This is less of a problem with larger datasets where each fold will contain sufficiently many observations to be approximately representative of the uderlying distribution. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da883efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv_specification = LassoCV(cv=5, random_state=0, max_iter=10000)\n",
    "lasso_cv_specification.fit(X_cv_train, y_cv_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9325d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = lasso_cv_specification.alpha_\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c462f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_lasso = Lasso(alpha = best_alpha)\n",
    "cv_lasso.fit(X_cv_train, y_cv_train)\n",
    "cv_lasso_train_pred = lasso_reg.predict(X_cv_train)\n",
    "cv_lasso_test_pred = lasso_reg.predict(X_cv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7eef5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the non-zero coefficients and see what variables are being selected\n",
    "features_df.iloc[:,cv_lasso.coef_!=0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training R^2 (in a slightly different way for practice)\n",
    "train_mse = round(mean_squared_error(y_cv_train, cv_lasso_train_pred), 6)\n",
    "print(\"The training set mean squared error in the cross validation approach is:\", train_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65950646",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_r2 = round(cv_lasso.score(X_cv_train, y_cv_train)*100, 2)\n",
    "print(\"The training set R-Squared in the cross validation approach is:\", train_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e0b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate performance in test set\n",
    "test_mse = round(mean_squared_error(y_cv_test, cv_lasso_test_pred), 6)\n",
    "print(\"The test set mean squared error in the cross validation approach is:\", test_mse)\n",
    "\n",
    "test_r2 = round(cv_lasso.score(X_cv_test, y_cv_test)*100, 2)\n",
    "print(\"The test set R-Squared in the cross validation approach is:\", test_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c168309",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_dict = {\n",
    "    \"Feature\": list(features_df.columns),\n",
    "    \"Coefficient\": cv_lasso.coef_\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367aa898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame and sort by coefficient magnitude in descending order\n",
    "coefficients_df = pd.DataFrame(coef_dict)\n",
    "coefficients_df = coefficients_df.reindex(coefficients_df[\"Coefficient\"].abs().sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f8bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250994bb",
   "metadata": {},
   "source": [
    "## 6. Discussion Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a00cd8",
   "metadata": {},
   "source": [
    "### 6.1 What is the bias-variance tradeoff? Why is it relevant to machine learning poblems like this one?\n",
    "You can decrease bias, or the degree to which the fitted values of your model differ from the actual values, by incorporating more information, or predictors, into your model. Doing this in a sample, there is some degree to which you are capturing the way the real world works, and some degree to which you are explaining random variation in your particular sample. If you include lots of predictors that explain random variation, then your predcition model will not be consitent when you use it on other samples since those new samples will have random variation that looks very different from your original data. This is is the bias-variance trade-off. This is relevant to machine learning because we care about creating prediction models that capture the way the world truly works, and that give us consistent and accurate predictions across models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565e54e8",
   "metadata": {},
   "source": [
    "### 6.2 Define overfitting, and why it matters for machine learning. How can we address it?\n",
    "Overfitting is tailoring your estimate of a model to fit the idiosyncracies of a particular dataset that are a product of random error or potentially measurment error rather than the underlying data generating process. In the context of diabetes, we might think of diabetes as caused primarily by diet, exercise, and genetic factors. However, we may, by random chance, get a sample of data on diabetes prevalence that happens to contain lots of individuals who are accounttants and have diabetes. This does not mean that diabetes is caused by becoming an accountant. This is just a feature of the particular sample we happened to draw. Overfitting in this particular example means that we fit a model that uses being an accountant as a strong predictor of diabetes. \n",
    "\n",
    "In machine learning, we are primarily concenred with prediction. In our toy example, our model is unlikely to predict diabetes well when we apply it to other datasets because being an accountant has no direct link with diabetes. In practice, the random noise we get in any given dataset is likely to be more subtle than the accountant example. Nevertheless, our algorithm may chase down this random noise rather than focusing on information that truly predicts our outcome.\n",
    "\n",
    "One simple way to address overfitting is to average models over many many samples. If we take multiple samples, the idisyncracies from any one sample will eventually cancel each other out. Some samples may have lots of accountants with diabetes leading to a model that uses being an accountant as an improtant predictor of diabetes. Other samples may have very few accountants and will conclude this is not an improtant predictor. Other samples may have lots of accountants who do not have diabetes and conclude being an accountant is a protective factor. By averaging these the predictions across these samples together, the noise (being an accountant) will not being given much weight. \n",
    "\n",
    "Another way to prevent overfitting is to only place weight on predictors that expalin a significant amount of variation in the outcome, this is regularization. Being an accountant may explain some variation in diabetes in a given sample, but it is unlikely to explain that much. Regularization will help to produce a model where predictors that explain small pockets of noise are given little to no improtance for prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62bf171",
   "metadata": {},
   "source": [
    "### 6.3 Discussion of analysis\n",
    "According to the algorithm, predictors like the obesity and physcial activity number, as well as whether the county is in the South or Southeast are indicative of higher diabetes rates. It would be natural to pilot the programs in these areas. However, we know the actual diabetes rate and we are interested in preventing diabetes. It would likely be best to prioritize piloting the program in areas where obesity and physical inactivity are increasing, but where diabetes is not necessarily high as these factors may be leading predictors of diabetes. However, the algorithm does not tell us this and certainly does not tell us the causal relationship between diabetes and these factors. \n",
    "\n",
    "Race, gender, and age are important predictors of diabetes rates according to the algorithm, the ommision or inclusion of these predictors would likely change how accurate the algorithm is at predicting diabetes rates. While this algorithm is a good start for predciting diabetes rates, it does not predict future diabetes rates or factors that cause diabetes that might be changeable through the prevention program. Consequently, it would be better to run this algorithm on panel data and to know more about the design of the prevention problem. That being said, if we truly need to a contemporaneous predictor of diabetes rates, this algorithm may be our best availble option."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
